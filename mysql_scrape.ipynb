{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into MySQL successfully!\n",
      "Data inserted into PostgreSQL successfully!\n",
      "Scraping complete! Data saved to MySQL and PostgreSQL databases.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Database Connection Details\n",
    "MYSQL_DB = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "    \"database\": \"scraping\",\n",
    "}\n",
    "\n",
    "POSTGRESQL_DB = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"password\",\n",
    "    \"database\": \"scraping\",\n",
    "}\n",
    "\n",
    "# Setup Selenium WebDriver with Headless Mode\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\")\n",
    "\n",
    "service = Service(\"chromedriver.exe\")  # Ensure correct chromedriver path\n",
    "\n",
    "# Load the CSV file with Twitter links\n",
    "df = pd.read_csv(\"twitter_links.csv\", header=None)\n",
    "df.columns = [\"twitter_link\"]\n",
    "df[\"username\"] = df[\"twitter_link\"].str.extract(r\"twitter\\.com\\/@?([^\\/\\s]+)\")\n",
    "\n",
    "# List to store profile data\n",
    "profile_data = []\n",
    "\n",
    "# Function to scrape Twitter profiles\n",
    "def scrape_twitter_profile(username, driver):\n",
    "    if pd.isna(username):  # If there's no username, mark as \"No Profile Present\"\n",
    "        return [\"No Profile Present\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\"]\n",
    "    \n",
    "    twitter_url = f\"https://twitter.com/{username}\"\n",
    "    driver.get(twitter_url)\n",
    "    \n",
    "    try:\n",
    "        if username == df[\"username\"].iloc[0]:  # First row\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div[data-testid=\"UserDescription\"], span[data-testid=\"UserLocation\"]')))\n",
    "        else:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div[data-testid=\"UserDescription\"], span[data-testid=\"UserLocation\"]'))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Timeout or CAPTCHA issue with {username}: {e}\")\n",
    "        return [username, \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\"]\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    def extract_data(selector, attribute=\"text\"):\n",
    "        try:\n",
    "            element = soup.select_one(selector)\n",
    "            return element.text.strip() if attribute == \"text\" else element[attribute]\n",
    "        except (AttributeError, TypeError):\n",
    "            return \"N/A\"\n",
    "\n",
    "    bio = extract_data('div[data-testid=\"UserDescription\"]')\n",
    "    followers = extract_data(f'a[href=\"/{username}/followers\"] span')\n",
    "    following = extract_data(f'a[href=\"/{username}/following\"] span')\n",
    "    location = extract_data('span[data-testid=\"UserLocation\"]')\n",
    "    website = extract_data('a[data-testid=\"UserUrl\"]', \"href\")\n",
    "\n",
    "    return [username, bio, followers, following, location, website]\n",
    "\n",
    "# Use a single WebDriver instance for efficiency\n",
    "with webdriver.Chrome(service=service, options=chrome_options) as driver:\n",
    "    for username in df[\"username\"]:\n",
    "        profile_data.append(scrape_twitter_profile(username, driver))\n",
    "        time.sleep(random.uniform(5,10))  # Random delay to avoid rate limits\n",
    "\n",
    "# Convert data to Pandas DataFrame\n",
    "output_df = pd.DataFrame(profile_data, columns=[\"Username\", \"Bio\", \"Followers\", \"Following\", \"Location\", \"Website\"])\n",
    "\n",
    "# Function to insert data into MySQL\n",
    "def insert_into_mysql(df):\n",
    "    try:\n",
    "        engine = create_engine(f\"mysql+mysqlconnector://{MYSQL_DB['user']}:{MYSQL_DB['password']}@{MYSQL_DB['host']}/{MYSQL_DB['database']}\")\n",
    "        df.to_sql(name=\"twitter_profiles\", con=engine, if_exists=\"replace\", index=False)\n",
    "        print(\"Data inserted into MySQL successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"MySQL Error: {e}\")\n",
    "\n",
    "# Function to insert data into PostgreSQL\n",
    "def insert_into_postgresql(df):\n",
    "    try:\n",
    "        engine = create_engine(f\"postgresql+psycopg2://{POSTGRESQL_DB['user']}:{POSTGRESQL_DB['password']}@{POSTGRESQL_DB['host']}/{POSTGRESQL_DB['database']}\")\n",
    "        df.to_sql(name=\"twitter_profiles\", con=engine, if_exists=\"replace\", index=False)\n",
    "        print(\"Data inserted into PostgreSQL successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"PostgreSQL Error: {e}\")\n",
    "\n",
    "# Insert data into both databases\n",
    "insert_into_mysql(output_df)\n",
    "insert_into_postgresql(output_df)\n",
    "\n",
    "print(\"Scraping complete! Data saved to MySQL and PostgreSQL databases.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
